"""
Phase 2A: å°çº¢ä¹¦å¹³å°ç ”ç©¶èŠ‚ç‚¹
ä½¿ç”¨ Playwright Deep Search è¿›è¡ŒçœŸå®çš„æµè§ˆå™¨æœç´¢
"""
import asyncio
import json
from datetime import datetime
from pathlib import Path

from ..state import XHSState
from ..tools.deep_search_playwright import XiaohongshuDeepSearch


async def research_xhs_node(state: XHSState) -> dict:
    """
    å°çº¢ä¹¦å¹³å°ç ”ç©¶èŠ‚ç‚¹ - ä½¿ç”¨ Playwright Deep Search

    æ‰§è¡Œæµç¨‹ï¼š
    1. ä½¿ç”¨çœŸå®æµè§ˆå™¨æ§åˆ¶æœç´¢å°çº¢ä¹¦
    2. æ·±åº¦é˜…è¯»å¸–å­è¯¦æƒ…å’Œè¯„è®ºåŒº
    3. æå–å…·ä½“å®ä½“ï¼ˆå…¬å¸åã€ä»·æ ¼ã€æ—¶é—´ç­‰ï¼‰
    4. è¯„ä¼°å¯ä¿¡åº¦
    5. ä¿å­˜ç»“æ„åŒ–æ•°æ®

    Args:
        state: å½“å‰å·¥ä½œæµçŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—æ®µï¼ˆxhs_research, xhs_research_completed, logsï¼‰
    """
    topic = state["topic"]
    target_audience = state.get("target_audience", "")

    print("\n" + "=" * 60)
    print(f"ğŸ” Phase 2A: å°çº¢ä¹¦å¹³å°ç ”ç©¶")
    print("=" * 60)
    print(f"ä¸»é¢˜: {topic}")
    print(f"å—ä¼—: {target_audience}")
    print(f"æ–¹æ³•: Playwright Deep Searchï¼ˆçœŸå®æµè§ˆå™¨ï¼‰")
    print("=" * 60)

    # åˆ›å»ºæ·±åº¦æœç´¢å¼•æ“ï¼ˆä½¿ç”¨ç‹¬ç«‹çš„ session ç›®å½•ï¼‰
    searcher = XiaohongshuDeepSearch(user_data_dir="./browser-sessions/xiaohongshu")

    # æ‰§è¡Œæ·±åº¦æœç´¢
    results = await searcher.search(
        keyword=topic,
        max_results=10,  # æ·±åº¦é˜…è¯»10ç¯‡å¸–å­
        read_comments=True  # è¯»å–è¯„è®ºåŒº
    )

    # è½¬æ¢ä¸ºç ”ç©¶æ•°æ®æ ¼å¼
    research_data = _convert_to_research_format(results, topic)

    # ä¿å­˜åˆ°æ–‡ä»¶
    project_dir = Path(state["project_dir"])
    xhs_research_path = project_dir / "xiaohongshu-research.json"

    with open(xhs_research_path, "w", encoding="utf-8") as f:
        json.dump(research_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å°çº¢ä¹¦ç ”ç©¶å®Œæˆ")
    print(f"   - æ·±åº¦é˜…è¯»: {len(results)} ç¯‡å¸–å­")
    print(f"   - æ•°æ®ç‚¹: {research_data['data_points']} ä¸ª")
    print(f"   - å®ä½“: {len(research_data['entities'])} ä¸ª")
    print(f"   - ä¿å­˜è·¯å¾„: {xhs_research_path}")

    # è®°å½•æ—¥å¿—
    log_message = f"[{datetime.now().isoformat()}] XHS Deep Search completed: {len(results)} posts, {research_data['data_points']} data points"

    return {
        "xhs_research": research_data,
        "xhs_research_completed": True,
        "logs": [log_message]
    }


def _convert_to_research_format(results: list, topic: str) -> dict:
    """
    å°† Deep Search ç»“æœè½¬æ¢ä¸ºç ”ç©¶æ•°æ®æ ¼å¼

    Args:
        results: DeepSearchResult åˆ—è¡¨
        topic: æœç´¢ä¸»é¢˜

    Returns:
        ç ”ç©¶æ•°æ®å­—å…¸
    """
    # æ”¶é›†æ‰€æœ‰å®ä½“
    all_entities = []
    for result in results:
        for entity in result.extracted_entities:
            all_entities.append({
                "name": entity.get("value", ""),
                "type": entity.get("type", "unknown"),
                "source": result.title,
                "url": result.url,
                "credibility": result.credibility
            })

    # æ„å»ºæ¡ˆä¾‹
    cases = []
    for result in results:
        if result.content:
            cases.append({
                "title": result.title,
                "author": result.author,
                "content": result.content[:300],  # æˆªå–å‰300å­—
                "likes": result.likes,
                "comments": result.comments_count,
                "url": result.url,
                "credibility": result.credibility
            })

    # æå–å…³é”®è¯ï¼ˆä»æ ‡é¢˜ä¸­æå–ï¼‰
    keywords = set()
    for result in results:
        # ç®€å•çš„å…³é”®è¯æå–ï¼šåˆ†è¯å¹¶è¿‡æ»¤
        words = result.title.split()
        for word in words:
            if len(word) >= 2:
                keywords.add(word)

    # è®¡ç®—æ€»ä½“å¯ä¿¡åº¦
    if results:
        high_count = sum(1 for r in results if r.credibility == "high")
        medium_count = sum(1 for r in results if r.credibility == "medium")

        if high_count > len(results) / 2:
            overall_credibility = "high"
        elif high_count + medium_count > len(results) / 2:
            overall_credibility = "medium"
        else:
            overall_credibility = "low"
    else:
        overall_credibility = "low"

    # ç”Ÿæˆç ”ç©¶æ€»ç»“
    summary = f"ä»å°çº¢ä¹¦å¹³å°æ·±åº¦æœç´¢'{topic}'ï¼Œå…±é˜…è¯»{len(results)}ç¯‡å¸–å­ï¼Œæå–{len(all_entities)}ä¸ªå…·ä½“å®ä½“ã€‚"

    return {
        "summary": summary,
        "entities": all_entities,
        "cases": cases[:10],  # æœ€å¤š10ä¸ªæ¡ˆä¾‹
        "keywords": list(keywords)[:20],  # æœ€å¤š20ä¸ªå…³é”®è¯
        "credibility": overall_credibility,
        "data_points": len(results),
        "total_likes": sum(r.likes for r in results),
        "total_comments": sum(r.comments_count for r in results),
        "platform": "xiaohongshu",
        "search_method": "playwright_deep_search",
        "timestamp": datetime.now().isoformat()
    }
