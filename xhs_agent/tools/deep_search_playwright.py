"""
Deep Search - Playwright å®ç°
åŸºäºçœŸå®æµè§ˆå™¨æ§åˆ¶çš„æ·±åº¦æœç´¢å¼•æ“
"""
import re
import json
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from .playwright_browser import PlaywrightBrowserManager


@dataclass
class DeepSearchResult:
    """æ·±åº¦æœç´¢ç»“æœ"""
    platform: str
    title: str
    content: str
    author: str
    url: str

    # äº’åŠ¨æ•°æ®
    likes: int = 0
    comments_count: int = 0
    shares: int = 0

    # æ—¶é—´ä¿¡æ¯
    publish_time: str = ""

    # æ·±åº¦æ•°æ®
    comments: List[Dict] = field(default_factory=list)  # è¯„è®ºåŒºæ•°æ®
    extracted_entities: List[Dict] = field(default_factory=list)  # æå–çš„å®ä½“
    images: List[str] = field(default_factory=list)

    # å…ƒæ•°æ®
    credibility: str = "medium"  # high/medium/low
    search_depth: int = 1  # æœç´¢æ·±åº¦ï¼ˆ1=åˆ—è¡¨é¡µï¼Œ2=è¯¦æƒ…é¡µï¼Œ3=è¯„è®ºæ·±åº¦åˆ†æï¼‰

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "platform": self.platform,
            "title": self.title,
            "content": self.content,
            "author": self.author,
            "url": self.url,
            "likes": self.likes,
            "comments_count": self.comments_count,
            "shares": self.shares,
            "publish_time": self.publish_time,
            "comments": self.comments,
            "extracted_entities": self.extracted_entities,
            "images": self.images,
            "credibility": self.credibility,
            "search_depth": self.search_depth
        }


class XiaohongshuDeepSearch:
    """
    å°çº¢ä¹¦æ·±åº¦æœç´¢

    æ‰§è¡Œæµç¨‹ï¼š
    1. å¯¼èˆªåˆ°æœç´¢é¡µ
    2. è·å–æœç´¢ç»“æœåˆ—è¡¨
    3. ç‚¹å‡»è¿›å…¥è¯¦æƒ…é¡µ
    4. è¯»å–å®Œæ•´å†…å®¹
    5. æ»šåŠ¨å¹¶è¯»å–è¯„è®º
    6. æå–å…·ä½“å®ä½“
    7. è¿”å›ç»“æ„åŒ–æ•°æ®
    """

    def __init__(self, user_data_dir: str = "./browser-sessions/platform"):
        self.user_data_dir = user_data_dir

    async def search(
        self,
        keyword: str,
        max_results: int = 10,
        read_comments: bool = True
    ) -> List[DeepSearchResult]:
        """
        æ‰§è¡Œæ·±åº¦æœç´¢

        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤šè¿”å›ç»“æœæ•°
            read_comments: æ˜¯å¦è¯»å–è¯„è®ºåŒº

        Returns:
            æ·±åº¦æœç´¢ç»“æœåˆ—è¡¨
        """
        print(f"\nğŸ” å¼€å§‹å°çº¢ä¹¦æ·±åº¦æœç´¢: {keyword}")
        print(f"   ç›®æ ‡: æ·±åº¦é˜…è¯» {max_results} ç¯‡å¸–å­")

        results = []

        async with PlaywrightBrowserManager(self.user_data_dir) as browser:
            # Step 1: å¯¼èˆªåˆ°æœç´¢é¡µ
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
            print(f"\n   â†’ å¯¼èˆªåˆ°æœç´¢é¡µ...")
            await browser.navigate(search_url)
            await browser.wait(2000)

            # Step 2: ç­‰å¾…ç¬”è®°å¡ç‰‡åŠ è½½
            print(f"   â†’ ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
            notes = await browser.find_elements("a.cover.ld.mask", timeout=10000)

            if not notes:
                print(f"   âš ï¸  æœªæ‰¾åˆ°ç¬”è®°å¡ç‰‡ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨...")
                notes = await browser.find_elements(".note-item", timeout=5000)

            if not notes:
                print(f"   âŒ æœªæ‰¾åˆ°ä»»ä½•ç¬”è®°")
                return results

            print(f"   âœ… æ‰¾åˆ° {len(notes)} ä¸ªç¬”è®°")

            # Step 3: å…ˆæ”¶é›†æ‰€æœ‰ç¬”è®°é“¾æ¥
            note_urls = []
            for i, note_element in enumerate(notes[:max_results]):
                try:
                    note_url = await note_element.get_attribute("href")
                    if note_url:
                        # è¡¥å…¨ URL
                        if not note_url.startswith("http"):
                            note_url = "https://www.xiaohongshu.com" + note_url
                        note_urls.append(note_url)
                except Exception as e:
                    print(f"   âš ï¸  è·å–ç¬¬ {i+1} ä¸ªç¬”è®°é“¾æ¥å¤±è´¥: {e}")
                    continue

            print(f"   âœ… æ”¶é›†åˆ° {len(note_urls)} ä¸ªç¬”è®°é“¾æ¥")

            # Step 4: é€ä¸ªè®¿é—®ç¬”è®°è¯¦æƒ…
            for i, note_url in enumerate(note_urls):
                print(f"\n   ğŸ“– æ·±åº¦é˜…è¯»ç¬¬ {i+1}/{len(note_urls)} ç¯‡å¸–å­...")

                try:
                    print(f"      â†’ è®¿é—®è¯¦æƒ…é¡µ: {note_url}")

                    # å¯¼èˆªåˆ°è¯¦æƒ…é¡µ
                    await browser.navigate(note_url)
                    await browser.wait(2000)

                    # æå–è¯¦æƒ…é¡µæ•°æ®
                    result = await self._extract_detail_page(browser, read_comments)

                    if result:
                        result.url = note_url  # ç¡®ä¿URLæ­£ç¡®
                        results.append(result)

                except Exception as e:
                    print(f"      âŒ å¤„ç†ç¬¬ {i+1} ç¯‡å¸–å­æ—¶å‡ºé”™: {e}")
                    continue

        print(f"\nâœ… å°çº¢ä¹¦æœç´¢å®Œæˆ: æ·±åº¦é˜…è¯»äº† {len(results)} ç¯‡å¸–å­")
        return results

    async def _extract_detail_page(
        self,
        browser: PlaywrightBrowserManager,
        read_comments: bool
    ) -> Optional[DeepSearchResult]:
        """æå–è¯¦æƒ…é¡µæ•°æ® - ä¼˜åŒ–é€‰æ‹©å™¨"""

        try:
            # ç­‰å¾…å†…å®¹åŠ è½½
            await browser.wait(2000)

            # å°çº¢ä¹¦è¯¦æƒ…é¡µé€‰æ‹©å™¨ï¼ˆå¤šä¸ªå¤‡é€‰ï¼‰
            title_selectors = [
                "#detail-title",
                ".title",
                ".note-title",
                "[class*='title']",
                "h1"
            ]

            content_selectors = [
                "#detail-desc .note-text",
                "#detail-desc",
                ".desc",
                ".note-content",
                ".content",
                "[class*='desc']"
            ]

            author_selectors = [
                ".username",
                ".name",
                ".author-name",
                ".nickname",
                "[class*='author'] [class*='name']"
            ]

            # æå–æ ‡é¢˜ï¼ˆå°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼‰
            title = ""
            for selector in title_selectors:
                title = await browser.get_text(selector, timeout=2000)
                if title:
                    break

            # æå–å†…å®¹
            content = ""
            for selector in content_selectors:
                content = await browser.get_text(selector, timeout=2000)
                if content:
                    break

            # æå–ä½œè€…
            author = ""
            for selector in author_selectors:
                author = await browser.get_text(selector, timeout=1500)
                if author:
                    break

            # æå–äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è¯„è®ºï¼‰
            likes = 0
            comments_count = 0

            # å°è¯•æ‰¾ç‚¹èµæ•°
            likes_text = await browser.get_text("[class*='like'] [class*='count']", timeout=1500)
            if not likes_text:
                likes_text = await browser.get_text(".like-count", timeout=1000)
            likes = self._extract_number(likes_text)

            # å°è¯•æ‰¾è¯„è®ºæ•°
            comments_text = await browser.get_text("[class*='comment'] [class*='count']", timeout=1500)
            if not comments_text:
                comments_text = await browser.get_text(".comment-count", timeout=1000)
            comments_count = self._extract_number(comments_text)

            # è·å–å½“å‰ URL
            url = browser.get_current_url()

            print(f"      âœ… æ ‡é¢˜: {title[:30]}...")
            print(f"      âœ… ç‚¹èµ: {likes}, è¯„è®º: {comments_count}")

            # åˆ›å»ºç»“æœå¯¹è±¡
            result = DeepSearchResult(
                platform="xiaohongshu",
                title=title,
                content=content,
                author=author,
                url=url,
                likes=likes,
                comments_count=comments_count,
                search_depth=2  # å·²è¿›å…¥è¯¦æƒ…é¡µ
            )

            # Step 4: è¯»å–è¯„è®ºåŒº
            if read_comments and comments_count > 0:
                print(f"      ğŸ’¬ è¯»å–è¯„è®ºåŒº...")
                await browser.scroll_to_bottom(scroll_count=3, delay=1000)

                comments = await self._extract_comments(browser)
                result.comments = comments
                result.search_depth = 3  # å·²è¯»å–è¯„è®º

                print(f"      âœ… æå–äº† {len(comments)} æ¡è¯„è®º")

            # Step 5: æå–å…·ä½“å®ä½“
            print(f"      ğŸ“Š æå–å…·ä½“å®ä½“...")
            entities = self._extract_entities(result.content, result.comments)
            result.extracted_entities = entities

            if entities:
                print(f"      âœ… æå–äº† {len(entities)} ä¸ªå®ä½“")

            # Step 6: å¯ä¿¡åº¦è¯„ä¼°
            result.credibility = self._assess_credibility(result)

            return result

        except Exception as e:
            print(f"      âŒ æå–è¯¦æƒ…é¡µæ•°æ®æ—¶å‡ºé”™: {e}")
            return None

    async def _extract_comments(self, browser: PlaywrightBrowserManager) -> List[Dict]:
        """æå–è¯„è®ºåŒºæ•°æ®"""
        comments = []

        try:
            # æŸ¥æ‰¾è¯„è®ºå…ƒç´ 
            comment_elements = await browser.find_elements(".comment-item", timeout=3000)

            if not comment_elements:
                comment_elements = await browser.find_elements(".comment", timeout=3000)

            for elem in comment_elements[:20]:  # æœ€å¤š20æ¡è¯„è®º
                try:
                    # æå–è¯„è®ºè€…
                    author_elem = await elem.query_selector(".comment-author")
                    author = await author_elem.inner_text() if author_elem else ""

                    # æå–è¯„è®ºå†…å®¹
                    content_elem = await elem.query_selector(".comment-content")
                    content = await content_elem.inner_text() if content_elem else ""

                    if content:
                        comments.append({
                            "author": author,
                            "content": content,
                            "likes": 0
                        })

                except Exception as e:
                    continue

        except Exception as e:
            print(f"      âš ï¸  æå–è¯„è®ºå¤±è´¥: {e}")

        return comments

    def _extract_number(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        if not text:
            return 0

        # å¤„ç† "1.2ä¸‡" è¿™ç§æ ¼å¼
        if "ä¸‡" in text:
            match = re.search(r'([\d.]+)ä¸‡', text)
            if match:
                return int(float(match.group(1)) * 10000)

        # å¤„ç†æ™®é€šæ•°å­—
        match = re.search(r'(\d+)', text.replace(',', ''))
        return int(match.group(1)) if match else 0

    def _extract_entities(self, content: str, comments: List[Dict]) -> List[Dict]:
        """
        ä»å†…å®¹å’Œè¯„è®ºä¸­æå–å…·ä½“å®ä½“

        æå–ç›®æ ‡ï¼š
        - å…¬å¸å/åº—å
        - äº§å“å
        - ä»·æ ¼
        - åœ°å€
        - æ—¶é—´
        """
        entities = []

        # åˆå¹¶å†…å®¹å’Œè¯„è®º
        all_text = content + " " + " ".join([c.get("content", "") for c in comments])

        # æå–ä»·æ ¼ï¼ˆç®€å•æ­£åˆ™ï¼‰
        price_pattern = r'(\d+\.?\d*)[å…ƒå—](\/|æ¯)?'
        prices = re.findall(price_pattern, all_text)
        for price, _ in prices:
            entities.append({
                "type": "price",
                "value": f"{price}å…ƒ",
                "source": "content"
            })

        # æå–æ—¶é—´ï¼ˆå¹´æœˆï¼‰
        time_pattern = r'20\d{2}å¹´\d{1,2}æœˆ'
        times = re.findall(time_pattern, all_text)
        for time in times:
            entities.append({
                "type": "time",
                "value": time,
                "source": "content"
            })

        # å»é‡
        seen = set()
        unique_entities = []
        for entity in entities:
            key = f"{entity['type']}:{entity['value']}"
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)

        return unique_entities

    def _assess_credibility(self, result: DeepSearchResult) -> str:
        """
        è¯„ä¼°ä¿¡æ¯å¯ä¿¡åº¦

        è¯„ä¼°å› ç´ ï¼š
        - äº’åŠ¨æ•°æ®ï¼ˆé«˜èµ = æ›´å¯ä¿¡ï¼‰
        - æ˜¯å¦æœ‰å…·ä½“ç»†èŠ‚
        - è¯„è®ºåŒºæ˜¯å¦æœ‰éªŒè¯
        """
        score = 0

        # äº’åŠ¨æ•°æ®
        if result.likes > 1000:
            score += 2
        elif result.likes > 100:
            score += 1

        # æ˜¯å¦æœ‰è¯„è®ºéªŒè¯
        if len(result.comments) > 10:
            score += 1

        # æ˜¯å¦æœ‰å…·ä½“å®ä½“
        if len(result.extracted_entities) > 0:
            score += 2

        if score >= 4:
            return "high"
        elif score >= 2:
            return "medium"
        else:
            return "low"


class ZhihuDeepSearch:
    """çŸ¥ä¹æ·±åº¦æœç´¢"""

    def __init__(self, user_data_dir: str = "./browser-sessions/platform"):
        self.user_data_dir = user_data_dir

    async def search(
        self,
        keyword: str,
        max_results: int = 10
    ) -> List[DeepSearchResult]:
        """æ‰§è¡ŒçŸ¥ä¹æ·±åº¦æœç´¢"""

        print(f"\nğŸ” å¼€å§‹çŸ¥ä¹æ·±åº¦æœç´¢: {keyword}")

        results = []

        async with PlaywrightBrowserManager(self.user_data_dir) as browser:
            # å¯¼èˆªåˆ°æœç´¢é¡µ
            search_url = f"https://www.zhihu.com/search?type=content&q={keyword}"
            print(f"   â†’ å¯¼èˆªåˆ°æœç´¢é¡µ...")
            await browser.navigate(search_url)
            await browser.wait(2000)

            # æŸ¥æ‰¾æœç´¢ç»“æœ
            items = await browser.find_elements(".List-item", timeout=10000)

            if not items:
                print(f"   âŒ æœªæ‰¾åˆ°æœç´¢ç»“æœ")
                return results

            print(f"   âœ… æ‰¾åˆ° {len(items)} ä¸ªç»“æœ")

            for i in range(min(max_results, len(items))):
                print(f"\n   ğŸ“– æ·±åº¦é˜…è¯»ç¬¬ {i+1}/{max_results} ä¸ªå›ç­”...")

                try:
                    # é‡æ–°è·å–åˆ—è¡¨
                    items = await browser.find_elements(".List-item", timeout=5000)
                    if not items or i >= len(items):
                        continue

                    item = items[i]

                    # æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
                    title_link = await item.query_selector("h2 a")
                    if not title_link:
                        continue

                    item_url = await title_link.get_attribute("href")
                    if item_url:
                        # è¡¥å…¨URL
                        if item_url.startswith("//"):
                            item_url = "https:" + item_url
                        elif item_url.startswith("/"):
                            item_url = "https://www.zhihu.com" + item_url
                        elif not item_url.startswith("http"):
                            item_url = "https://www.zhihu.com/" + item_url

                    # è®¿é—®è¯¦æƒ…é¡µ
                    await browser.navigate(item_url)
                    await browser.wait(2000)

                    # æå–æ ‡é¢˜
                    title = await browser.get_text("h1.QuestionHeader-title", timeout=3000)

                    # æå–å›ç­”å†…å®¹
                    content = await browser.get_text(".RichContent-inner", timeout=3000)

                    # æå–ä½œè€…
                    author = await browser.get_text(".AuthorInfo-name", timeout=3000)

                    # æå–ç‚¹èµæ•°
                    likes_text = await browser.get_text(".VoteButton--up", timeout=3000)
                    likes = self._extract_number(likes_text)

                    results.append(DeepSearchResult(
                        platform="zhihu",
                        title=title,
                        content=content[:500],  # çŸ¥ä¹å›ç­”é€šå¸¸å¾ˆé•¿ï¼Œæˆªå–å‰500å­—
                        author=author,
                        url=item_url,
                        likes=likes,
                        search_depth=2
                    ))

                    # è¿”å›æœç´¢é¡µ
                    await browser.go_back()
                    await browser.wait(2000)

                except Exception as e:
                    print(f"      âŒ å¤„ç†å¤±è´¥: {e}")
                    continue

        print(f"\nâœ… çŸ¥ä¹æœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results

    def _extract_number(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        if not text:
            return 0
        match = re.search(r'(\d+)', text.replace(',', ''))
        return int(match.group(1)) if match else 0


class WeiboDeepSearch:
    """å¾®åšæ·±åº¦æœç´¢"""

    def __init__(self, user_data_dir: str = "./browser-sessions/platform"):
        self.user_data_dir = user_data_dir

    async def search(
        self,
        keyword: str,
        max_results: int = 10
    ) -> List[DeepSearchResult]:
        """æ‰§è¡Œå¾®åšæ·±åº¦æœç´¢"""

        print(f"\nğŸ” å¼€å§‹å¾®åšæ·±åº¦æœç´¢: {keyword}")

        results = []

        async with PlaywrightBrowserManager(self.user_data_dir) as browser:
            # å¯¼èˆªåˆ°æœç´¢é¡µ
            search_url = f"https://s.weibo.com/weibo?q={keyword}"
            print(f"   â†’ å¯¼èˆªåˆ°æœç´¢é¡µ...")
            await browser.navigate(search_url)
            await browser.wait(3000)

            # æŸ¥æ‰¾å¾®åšå¡ç‰‡
            cards = await browser.find_elements(".card-wrap", timeout=10000)

            if not cards:
                print(f"   âŒ æœªæ‰¾åˆ°å¾®åš")
                return results

            print(f"   âœ… æ‰¾åˆ° {len(cards)} æ¡å¾®åš")

            for i in range(min(max_results, len(cards))):
                print(f"\n   ğŸ“– è¯»å–ç¬¬ {i+1}/{max_results} æ¡å¾®åš...")

                try:
                    cards = await browser.find_elements(".card-wrap", timeout=5000)
                    if not cards or i >= len(cards):
                        continue

                    card = cards[i]

                    # æå–å†…å®¹
                    content_elem = await card.query_selector(".txt")
                    content = await content_elem.inner_text() if content_elem else ""

                    # æå–ä½œè€…
                    author_elem = await card.query_selector(".name")
                    author = await author_elem.inner_text() if author_elem else ""

                    # æå–ç‚¹èµæ•°
                    like_elem = await card.query_selector(".woo-like-count")
                    likes_text = await like_elem.inner_text() if like_elem else "0"
                    likes = self._extract_number(likes_text)

                    if content:
                        results.append(DeepSearchResult(
                            platform="weibo",
                            title=content[:50] + "...",  # å¾®åšæ²¡æœ‰æ ‡é¢˜ï¼Œç”¨å†…å®¹å¼€å¤´
                            content=content,
                            author=author,
                            url=browser.get_current_url(),
                            likes=likes,
                            search_depth=1
                        ))

                except Exception as e:
                    print(f"      âŒ å¤„ç†å¤±è´¥: {e}")
                    continue

        print(f"\nâœ… å¾®åšæœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results

    def _extract_number(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        if not text:
            return 0
        if "ä¸‡" in text:
            match = re.search(r'([\d.]+)ä¸‡', text)
            if match:
                return int(float(match.group(1)) * 10000)
        match = re.search(r'(\d+)', text.replace(',', ''))
        return int(match.group(1)) if match else 0


class TiebaDeepSearch:
    """è´´å§æ·±åº¦æœç´¢"""

    def __init__(self, user_data_dir: str = "./browser-sessions/platform"):
        self.user_data_dir = user_data_dir

    async def search(
        self,
        keyword: str,
        max_results: int = 10
    ) -> List[DeepSearchResult]:
        """æ‰§è¡Œè´´å§æ·±åº¦æœç´¢"""

        print(f"\nğŸ” å¼€å§‹è´´å§æ·±åº¦æœç´¢: {keyword}")

        results = []

        async with PlaywrightBrowserManager(self.user_data_dir) as browser:
            # å¯¼èˆªåˆ°æœç´¢é¡µ
            search_url = f"https://tieba.baidu.com/f/search/res?qw={keyword}"
            print(f"   â†’ å¯¼èˆªåˆ°æœç´¢é¡µ...")
            await browser.navigate(search_url)
            await browser.wait(3000)

            # æŸ¥æ‰¾å¸–å­
            posts = await browser.find_elements(".s_post", timeout=10000)

            if not posts:
                print(f"   âŒ æœªæ‰¾åˆ°å¸–å­")
                return results

            print(f"   âœ… æ‰¾åˆ° {len(posts)} ä¸ªå¸–å­")

            for i in range(min(max_results, len(posts))):
                print(f"\n   ğŸ“– è¯»å–ç¬¬ {i+1}/{max_results} ä¸ªå¸–å­...")

                try:
                    posts = await browser.find_elements(".s_post", timeout=5000)
                    if not posts or i >= len(posts):
                        continue

                    post = posts[i]

                    # æå–æ ‡é¢˜
                    title_elem = await post.query_selector(".post_title")
                    title = await title_elem.inner_text() if title_elem else ""

                    # æå–ä½œè€…
                    author_elem = await post.query_selector(".tb_icon_author")
                    author = await author_elem.inner_text() if author_elem else ""

                    # æå–æ‘˜è¦
                    content_elem = await post.query_selector(".post_summary")
                    content = await content_elem.inner_text() if content_elem else ""

                    if title:
                        results.append(DeepSearchResult(
                            platform="tieba",
                            title=title,
                            content=content,
                            author=author,
                            url=browser.get_current_url(),
                            search_depth=1
                        ))

                except Exception as e:
                    print(f"      âŒ å¤„ç†å¤±è´¥: {e}")
                    continue

        print(f"\nâœ… è´´å§æœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results


class MultiPlatformDeepSearch:
    """è·¨å¹³å°æ·±åº¦æœç´¢"""

    def __init__(self, user_data_dir: str = "./browser-sessions"):
        self.user_data_dir = user_data_dir
        # ä¸ºæ¯ä¸ªå¹³å°ä½¿ç”¨ç‹¬ç«‹çš„ session ç›®å½•ï¼ˆé¿å…å¹¶è¡Œå†²çªï¼‰
        self.searchers = {
            "xiaohongshu": XiaohongshuDeepSearch(f"{user_data_dir}/xiaohongshu"),
            "zhihu": ZhihuDeepSearch(f"{user_data_dir}/zhihu"),
            "weibo": WeiboDeepSearch(f"{user_data_dir}/weibo"),
            "tieba": TiebaDeepSearch(f"{user_data_dir}/tieba")
        }

    async def search_all_platforms(
        self,
        keyword: str,
        platforms: Optional[List[str]] = None,
        max_results_per_platform: int = 10
    ) -> Dict[str, List[DeepSearchResult]]:
        """
        è·¨å¹³å°æ·±åº¦æœç´¢

        Args:
            keyword: æœç´¢å…³é”®è¯
            platforms: å¹³å°åˆ—è¡¨ï¼ˆé»˜è®¤æ‰€æœ‰ï¼‰
            max_results_per_platform: æ¯ä¸ªå¹³å°æœ€å¤šç»“æœæ•°

        Returns:
            {å¹³å°å: [æ·±åº¦æœç´¢ç»“æœ]}
        """
        if platforms is None:
            platforms = ["xiaohongshu", "zhihu", "weibo", "tieba"]

        all_results = {}

        print("=" * 60)
        print(f"ğŸš€ å¼€å§‹è·¨å¹³å°æ·±åº¦æœç´¢: {keyword}")
        print(f"   å¹³å°: {', '.join(platforms)}")
        print("=" * 60)

        for platform in platforms:
            try:
                if platform not in self.searchers:
                    print(f"\nâš ï¸  æœªçŸ¥å¹³å°: {platform}")
                    continue

                searcher = self.searchers[platform]
                results = await searcher.search(keyword, max_results_per_platform)
                all_results[platform] = results

            except Exception as e:
                print(f"\nâŒ {platform} æœç´¢å¤±è´¥: {e}")
                all_results[platform] = []

        # ç»Ÿè®¡æ€»ç»“
        total_results = sum(len(r) for r in all_results.values())
        print("\n" + "=" * 60)
        print(f"âœ… è·¨å¹³å°æ·±åº¦æœç´¢å®Œæˆ")
        print(f"   æ€»è®¡: {total_results} æ¡æ·±åº¦ç»“æœ")
        for platform, results in all_results.items():
            print(f"   - {platform}: {len(results)} æ¡")
        print("=" * 60)

        return all_results


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

async def demo_xiaohongshu_search():
    """æ¼”ç¤ºå°çº¢ä¹¦æ·±åº¦æœç´¢"""

    print("=" * 60)
    print("å°çº¢ä¹¦æ·±åº¦æœç´¢æ¼”ç¤º")
    print("=" * 60)

    searcher = XiaohongshuDeepSearch()

    results = await searcher.search(
        keyword="è¥¿å®‰å…¬å¸é¿å‘",
        max_results=3,
        read_comments=True
    )

    print(f"\næ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

    for i, result in enumerate(results):
        print(f"\nç»“æœ {i+1}:")
        print(f"  æ ‡é¢˜: {result.title}")
        print(f"  ä½œè€…: {result.author}")
        print(f"  ç‚¹èµ: {result.likes}")
        print(f"  è¯„è®º: {result.comments_count}")
        print(f"  å¯ä¿¡åº¦: {result.credibility}")
        print(f"  å®ä½“: {len(result.extracted_entities)} ä¸ª")
        print(f"  URL: {result.url}")

    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ")
    print("=" * 60)


async def demo_multi_platform_search():
    """æ¼”ç¤ºè·¨å¹³å°æ·±åº¦æœç´¢"""

    print("=" * 60)
    print("è·¨å¹³å°æ·±åº¦æœç´¢æ¼”ç¤º")
    print("=" * 60)

    searcher = MultiPlatformDeepSearch()

    all_results = await searcher.search_all_platforms(
        keyword="è¥¿å®‰å…¬å¸é¿å‘",
        platforms=["xiaohongshu", "zhihu"],
        max_results_per_platform=3
    )

    for platform, results in all_results.items():
        print(f"\n{platform} - {len(results)} ä¸ªç»“æœ:")
        for i, result in enumerate(results):
            print(f"  {i+1}. {result.title[:50]}...")

    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    import asyncio

    print("""
    å°çº¢ä¹¦æ·±åº¦æœç´¢å¼•æ“

    ç‰¹ç‚¹ï¼š
    1. çœŸå®æµè§ˆå™¨æ§åˆ¶
    2. æ·±åº¦æŒ–æ˜ï¼šæœç´¢ â†’ è¯¦æƒ… â†’ è¯„è®º
    3. å®ä½“æå–ï¼šå…¬å¸åã€ä»·æ ¼ã€æ—¶é—´
    4. å¯ä¿¡åº¦è¯„ä¼°

    ä½¿ç”¨å‰æï¼š
    - éœ€è¦å…ˆç™»å½•å°çº¢ä¹¦è´¦å·
    - è¿è¡Œ: python scripts/login_platforms.py
    """)

    # asyncio.run(demo_xiaohongshu_search())
